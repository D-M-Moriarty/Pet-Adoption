{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6b63ad26c7cf91dfbb3ab3163d7494843f568cea"
   },
   "source": [
    "In this kernel I want to demonstrate how to extract features from the pet images using a pretrained network. Since there are often none or multiple images of different resoltuions and aspect ratio I make the following preprocessing steps:\n",
    "\n",
    "- Take only profile picture (if existing else black)\n",
    "- pad to square aspect ratio\n",
    "- resize to 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "train_df = pd.read_csv('../pet_data/train.csv')\n",
    "img_size = 256\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "e8a7d78a79a1ef646f5bf73fb6b4059ef5fc0771"
   },
   "outputs": [],
   "source": [
    "pet_ids = train_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "fe31dbcf0c682ade04a66e2dd19c39fd53cb8591"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.densenet import preprocess_input, DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "18987c2d130e6d25de4082cb19d1d2a152c9749e"
   },
   "outputs": [],
   "source": [
    "def resize_to_square(im):\n",
    "    old_size = im.shape[:2] # old_size is in (height, width) format\n",
    "    ratio = float(img_size)/max(old_size)\n",
    "    new_size = tuple([int(x*ratio) for x in old_size])\n",
    "    # new_size should be in (width, height) format\n",
    "    im = cv2.resize(im, (new_size[1], new_size[0]))\n",
    "    delta_w = img_size - new_size[1]\n",
    "    delta_h = img_size - new_size[0]\n",
    "    top, bottom = delta_h//2, delta_h-(delta_h//2)\n",
    "    left, right = delta_w//2, delta_w-(delta_w//2)\n",
    "    color = [0, 0, 0]\n",
    "    new_im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT,value=color)\n",
    "    return new_im\n",
    "\n",
    "def load_image(path, pet_id):\n",
    "    image = cv2.imread(f'{path}{pet_id}-1.jpg')\n",
    "    new_image = resize_to_square(image)\n",
    "    new_image = preprocess_input(new_image)\n",
    "    return new_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "49a2e974500ebdef071054d74a5cc96baf79e0cd"
   },
   "source": [
    "Lets define our model for feature extraction. Normally DenseNet121 would output 1024 features after GlobalAveragePooling. To further narrow it down, I again pool 4 features each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9f761b784b4a81feed294b3b713510193edc773f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "29089792/29084464 [==============================] - 2s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import GlobalAveragePooling2D, Input, Lambda, AveragePooling1D\n",
    "import keras.backend as K\n",
    "inp = Input((256,256,3))\n",
    "backbone = DenseNet121(input_tensor = inp, include_top = False)\n",
    "x = backbone.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Lambda(lambda x: K.expand_dims(x,axis = -1))(x)\n",
    "x = AveragePooling1D(4)(x)\n",
    "out = Lambda(lambda x: x[:,:,0])(x)\n",
    "\n",
    "m = Model(inp,out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "039ac2cb53f2c43465e25160c31f9b8a74246d56"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fa3b2bcd4f40c6a2df6c8d98073fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=938), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "features = {}\n",
    "for b in tqdm_notebook(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/train_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "72e7fc11b1ecf6873b959d3577496c364919c85b"
   },
   "outputs": [],
   "source": [
    "train_feats = pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7e94c0a8b07ed0c8b46ba953636f2398a546d5c"
   },
   "source": [
    "We save the features as a csv to disk, so others can link and join the data frame with their train.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "2c676304977cfb548356c32f09f06dbee5f6228b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>246</th>\n",
       "      <th>247</th>\n",
       "      <th>248</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>86e1089a3</th>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.200123</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.011279</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.114432</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>0.03007</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0.071165</td>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.055685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6296e909a</th>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.200123</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.011279</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.114432</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>0.03007</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0.071165</td>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.055685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3422e4906</th>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.200123</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.011279</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.114432</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>0.03007</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0.071165</td>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.055685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5842f1ff5</th>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.200123</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.011279</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.114432</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>0.03007</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0.071165</td>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.055685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>850a43f90</th>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.200123</td>\n",
       "      <td>0.034662</td>\n",
       "      <td>0.011279</td>\n",
       "      <td>0.221239</td>\n",
       "      <td>0.002751</td>\n",
       "      <td>0.001853</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.114432</td>\n",
       "      <td>0.029524</td>\n",
       "      <td>...</td>\n",
       "      <td>0.017692</td>\n",
       "      <td>0.004894</td>\n",
       "      <td>0.045288</td>\n",
       "      <td>0.03007</td>\n",
       "      <td>0.013002</td>\n",
       "      <td>0.045238</td>\n",
       "      <td>0.051465</td>\n",
       "      <td>0.071165</td>\n",
       "      <td>0.063994</td>\n",
       "      <td>0.055685</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 256 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5    \\\n",
       "86e1089a3  0.001913  0.200123  0.034662  0.011279  0.221239  0.002751   \n",
       "6296e909a  0.001913  0.200123  0.034662  0.011279  0.221239  0.002751   \n",
       "3422e4906  0.001913  0.200123  0.034662  0.011279  0.221239  0.002751   \n",
       "5842f1ff5  0.001913  0.200123  0.034662  0.011279  0.221239  0.002751   \n",
       "850a43f90  0.001913  0.200123  0.034662  0.011279  0.221239  0.002751   \n",
       "\n",
       "                6         7         8         9    ...       246       247  \\\n",
       "86e1089a3  0.001853  0.004002  0.114432  0.029524  ...  0.017692  0.004894   \n",
       "6296e909a  0.001853  0.004002  0.114432  0.029524  ...  0.017692  0.004894   \n",
       "3422e4906  0.001853  0.004002  0.114432  0.029524  ...  0.017692  0.004894   \n",
       "5842f1ff5  0.001853  0.004002  0.114432  0.029524  ...  0.017692  0.004894   \n",
       "850a43f90  0.001853  0.004002  0.114432  0.029524  ...  0.017692  0.004894   \n",
       "\n",
       "                248      249       250       251       252       253  \\\n",
       "86e1089a3  0.045288  0.03007  0.013002  0.045238  0.051465  0.071165   \n",
       "6296e909a  0.045288  0.03007  0.013002  0.045238  0.051465  0.071165   \n",
       "3422e4906  0.045288  0.03007  0.013002  0.045238  0.051465  0.071165   \n",
       "5842f1ff5  0.045288  0.03007  0.013002  0.045238  0.051465  0.071165   \n",
       "850a43f90  0.045288  0.03007  0.013002  0.045238  0.051465  0.071165   \n",
       "\n",
       "                254       255  \n",
       "86e1089a3  0.063994  0.055685  \n",
       "6296e909a  0.063994  0.055685  \n",
       "3422e4906  0.063994  0.055685  \n",
       "5842f1ff5  0.063994  0.055685  \n",
       "850a43f90  0.063994  0.055685  \n",
       "\n",
       "[5 rows x 256 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feats.to_csv('train_img_features.csv')\n",
    "train_feats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "acae2bcbc492656648ece4ef5a3fd2d18afedffc"
   },
   "source": [
    "and repeat the procedure again for test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "d371c9e17eabaedc338fe9d46fdb80776792e2e4"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'../input/test/test.csv' does not exist: b'../input/test/test.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3cb98ce4c0af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/test/test.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'../input/test/test.csv' does not exist: b'../input/test/test.csv'"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('../input/test/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f7a13fc482a3b10a6a4dfeed5ee87175c923baf3"
   },
   "outputs": [],
   "source": [
    "pet_ids = test_df['PetID'].values\n",
    "n_batches = len(pet_ids) // batch_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9c7b0b302a2fc575b32fa6c81c11c5e2ecc5e2d7"
   },
   "outputs": [],
   "source": [
    "features = {}\n",
    "for b in tqdm_notebook(range(n_batches)):\n",
    "    start = b*batch_size\n",
    "    end = (b+1)*batch_size\n",
    "    batch_pets = pet_ids[start:end]\n",
    "    batch_images = np.zeros((len(batch_pets),img_size,img_size,3))\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        try:\n",
    "            batch_images[i] = load_image(\"../input/test_images/\", pet_id)\n",
    "        except:\n",
    "            pass\n",
    "    batch_preds = m.predict(batch_images)\n",
    "    for i,pet_id in enumerate(batch_pets):\n",
    "        features[pet_id] = batch_preds[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e8e7279ee2ebdbb59f424c90ae0430e5f7b40264"
   },
   "outputs": [],
   "source": [
    "test_feats = pd.DataFrame.from_dict(features, orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e6263a60f0af03217f9e970deb265add1e52e52"
   },
   "outputs": [],
   "source": [
    "test_feats.to_csv('test_img_features.csv')\n",
    "test_feats.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "56f317379a13a3fa94ea3c2c376dbb9186cd1f2a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
