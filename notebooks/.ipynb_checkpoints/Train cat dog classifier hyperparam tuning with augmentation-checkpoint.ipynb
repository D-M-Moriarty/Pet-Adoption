{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import ndimage\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras_preprocessing.image import ImageDataGenerator\n",
    "# from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications import VGG16\n",
    "from keras.applications import NASNetLarge, InceptionV3\n",
    "from keras.models import Model\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout, CuDNNLSTM, CuDNNGRU, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
    "from keras.optimizers import Adam, SGD, Nadam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from time import time\n",
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.preprocessing import image\n",
    "import os, shutil\n",
    "from IPython.display import display\n",
    "import os\n",
    "import cv2\n",
    "import math\n",
    "import json\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from hyperas.distributions import uniform\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "from hyperopt import Trials, STATUS_OK, tpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bread_labels = pd.read_csv('../pet_data/breed_labels.csv')\n",
    "color_labels = pd.read_csv('../pet_data/color_labels.csv')\n",
    "state_labels = pd.read_csv('../pet_data/state_labels.csv')\n",
    "test_sample_submission = pd.read_csv('../pet_data/test/sample_submission.csv')\n",
    "test_df = pd.read_csv('../pet_data/test/test.csv')\n",
    "train_df = pd.read_csv('../pet_data/train.csv')\n",
    "train_folder = '../pet_data/train_images/'\n",
    "test_folder = '../pet_data/test_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_target_dict = train_df.set_index('PetID')['AdoptionSpeed'].to_dict()\n",
    "name_target_dict_type = train_df.set_index('PetID')['Type'].to_dict()\n",
    "train_image_names = os.listdir('../pet_data/train_images')\n",
    "test_image_names = os.listdir('../pet_data/test_images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_samples(columns=4, rows=3):\n",
    "    fig=plt.figure(figsize=(5*columns, 3*rows))\n",
    "\n",
    "    for i in range(columns*rows):\n",
    "        image_path = train_image_names[i]\n",
    "        image_id = name_target_dict[image_path.split('-')[0]]\n",
    "        img = cv2.imread(f'../pet_data/train_images/{image_path}')\n",
    "        fig.add_subplot(rows, columns, i+1)\n",
    "        plt.title(image_id)\n",
    "        plt.imshow(img)\n",
    "\n",
    "display_samples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_dict = {'filename': [], 'class': [], 'type': []}\n",
    "\n",
    "for name in train_image_names:\n",
    "    short_name = name.split('-')[0]\n",
    "    label = name_target_dict[short_name]\n",
    "    t = name_target_dict_type[short_name]\n",
    "    \n",
    "    generator_dict['filename'].append(name)\n",
    "    generator_dict['class'].append(label)\n",
    "    generator_dict['type'].append(str(t))\n",
    "generator_df = pd.DataFrame(generator_dict)\n",
    "generator_df.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_df.groupby('type').size().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='confusion matrix', cmap=plt.cm.Blues):\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[: np.newaxis]\n",
    "        print('normalized confusion matrix')\n",
    "    else:\n",
    "        print('unnormalized confusion matrix')\n",
    "        \n",
    "    print(cm)\n",
    "    \n",
    "    thresh = cm.max() / 2\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                horizontalalignment='center',\n",
    "                color='white' if cm[i, j] > thresh else 'black'\n",
    "                )\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('model accuracy')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('model loss')\n",
    "    plt.xlabel('loss')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "def show_image_predictions(probabilities, direct):\n",
    "    for index, probability in enumerate(probabilities):\n",
    "        image_path = direct + \"/\" +test_generator.filenames[index]\n",
    "        img = mpimg.imread(image_path)\n",
    "    #     with open(TEST_FILE,\"a\") as fh:\n",
    "    #         fh.write(str(probability[0]) + \" for: \" + image_path + \"\\n\")\n",
    "        plt.imshow(img)\n",
    "        if probability > 0.5:\n",
    "            plt.title(\"%.2f\" % (probability[0]*100) + \"% dog in \" + image_path)\n",
    "        else:\n",
    "            plt.title(\"%.2f\" % ((1-probability[0])*100) + \"% cat\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "random_state = 42\n",
    "train, test = train_test_split(generator_df, test_size=0.2, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def plot_history(history):\n",
    "    plt.plot(history.history['acc'])\n",
    "    plt.plot(history.history['val_acc'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('model accuracy')\n",
    "    plt.xlabel('accuracy')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('model loss')\n",
    "    plt.xlabel('loss')\n",
    "    plt.legend(['train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.load_weights('model.h5')\n",
    "\n",
    "val_scores = model.evaluate_generator(\n",
    "    valid_generator,\n",
    "    STEP_SIZE_VALID,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "print('Validation loss:', val_scores[0])\n",
    "print('Validation accuracy:', val_scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen=ImageDataGenerator(rescale=1./255.)\n",
    "# cut off the last 3 rows as it messes \n",
    "# with the predictions due to the batch size not dividing evenly\n",
    "test_mod = test[:-3]\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "        test_mod, \n",
    "        '../pet_data/train_images/', \n",
    "        x_col='filename',\n",
    "        y_col=None, \n",
    "        target_size=(150, 150),\n",
    "        class_mode=None, \n",
    "        color_mode='rgb',\n",
    "        batch_size=16, \n",
    "        shuffle=False,\n",
    "        seed=2018,\n",
    "    )\n",
    "\n",
    "STEP_SIZE_TEST = test_generator.n//test_generator.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.reset()\n",
    "pred=model.predict_generator(\n",
    "    test_generator,\n",
    "    STEP_SIZE_TEST,\n",
    "    verbose=1\n",
    ")\n",
    "print('\\nTest loss:', pred[0])\n",
    "print('Test accuracy:', pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_dict = {'filename': [], 'prediction': []}\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for index, predict in enumerate(pred):\n",
    "    filename = test_generator.filenames[index]\n",
    "    if predict > 0.5:\n",
    "        prediction_dict['filename'].append(filename) \n",
    "        prediction_dict['prediction'].append(2) \n",
    "    else: \n",
    "        prediction_dict['filename'].append(filename) \n",
    "        prediction_dict['prediction'].append(1)\n",
    "\n",
    "predictions_df = pd.DataFrame(prediction_dict) \n",
    "\n",
    "y_true = test_mod.type.astype(int)\n",
    "y_pred = predictions_df.prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(\n",
    "    confusion_matrix(y_true, y_pred),\n",
    "    ['cat', 'dog']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report = classification_report(\n",
    "    y_true, \n",
    "    y_pred, \n",
    "    [1, 2]\n",
    ")\n",
    "print(report) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index, predict in enumerate(pred):\n",
    "    image_path = '../pet_data/train_images/' +test_generator.filenames[index]\n",
    "    img = mpimg.imread(image_path)\n",
    "    plt.imshow(img)\n",
    "    if predict > 0.5:\n",
    "        plt.title(\"%.2f\" % (predict[0]*100) + \"% cat\")\n",
    "    else:\n",
    "        plt.title(\"%.2f\" % ((1-predict[0])*100) + \"% dog\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Hyperas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    test_df = pd.read_csv('../pet_data/test/test.csv')\n",
    "    train_df = pd.read_csv('../pet_data/train.csv')\n",
    "    train_folder = '../pet_data/train_images/'\n",
    "    test_folder = '../pet_data/test_images/'\n",
    "    name_target_dict = train_df.set_index('PetID')['AdoptionSpeed'].to_dict()\n",
    "    name_target_dict_type = train_df.set_index('PetID')['Type'].to_dict()\n",
    "    train_image_names = os.listdir('../pet_data/train_images')\n",
    "    test_image_names = os.listdir('../pet_data/test_images')\n",
    "    generator_dict = {'filename': [], 'class': [], 'type': []}\n",
    "\n",
    "    for name in train_image_names:\n",
    "        short_name = name.split('-')[0]\n",
    "        label = name_target_dict[short_name]\n",
    "        t = name_target_dict_type[short_name]\n",
    "\n",
    "        generator_dict['filename'].append(name)\n",
    "        generator_dict['class'].append(label)\n",
    "        generator_dict['type'].append(str(t))\n",
    "        \n",
    "    generator_df = pd.DataFrame(generator_dict)\n",
    "    generator_df.head(8)\n",
    "    \n",
    "    random_state = 42\n",
    "    train, test = train_test_split(generator_df, test_size=0.2, random_state=random_state)\n",
    "    \n",
    "    train_datagen=ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        shear_range=0.1,  # set range for random shear\n",
    "        zoom_range=0.1,  # set range for random zoom\n",
    "        channel_shift_range=0.,  # set range for random channel shifts\n",
    "        # set mode for filling points outside the input boundaries\n",
    "        fill_mode='nearest',\n",
    "        cval=0.,  # value used for fill_mode = \"constant\"\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False,  # randomly flip images\n",
    "        # set rescaling factor (applied before any other transformation)\n",
    "        rescale=1/255.,\n",
    "        # set function that will be applied on each input\n",
    "        preprocessing_function=None,\n",
    "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
    "        validation_split=0.1\n",
    "    )\n",
    "    \n",
    "    train_generator = train_datagen.flow_from_dataframe(\n",
    "        train, \n",
    "        '../pet_data/train_images/', \n",
    "        x_col='filename',\n",
    "        y_col='type', \n",
    "        has_ext=True,  # If image extension is given in x_col\n",
    "        target_size=(150, 150), \n",
    "        color_mode='rgb',\n",
    "        class_mode='binary', \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        seed=2018,\n",
    "        subset='training'\n",
    "    )\n",
    "    valid_generator = train_datagen.flow_from_dataframe(\n",
    "        train, \n",
    "        '../pet_data/train_images/', \n",
    "        x_col='filename',\n",
    "        y_col='type', \n",
    "        has_ext=True,  # If image extension is given in x_col\n",
    "        target_size=(150, 150), \n",
    "        color_mode='rgb',\n",
    "        class_mode='binary', \n",
    "        batch_size=16, \n",
    "        shuffle=True, \n",
    "        seed=2018,\n",
    "        subset='validation'\n",
    "    )\n",
    "    \n",
    "    return train_generator, valid_generator, train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(train_generator, valid_generator):\n",
    "    model = models.Sequential()\n",
    "    \n",
    "    ks1_first = {{choice([2, 3, 4, 5, 8, 10])}}\n",
    "    ks1_second = {{choice([2, 3, 4, 5, 8, 10])}}\n",
    "    \n",
    "    ks2_first = {{choice([2, 3, 4, 5, 8, 10])}}\n",
    "    ks2_second = {{choice([2, 3, 4, 5, 8, 10])}}\n",
    "    \n",
    "    input_shape=(150, 150, 3)\n",
    "    \n",
    "    model.add(Conv2D(filters=( {{choice([1, 2, 3, 4, 5, 8, 12])}} ), \n",
    "                     kernel_size=(ks1_first, ks1_second),\n",
    "                     input_shape=input_shape, \n",
    "                     padding='same',\n",
    "                     kernel_initializer='TruncatedNormal',\n",
    "                     activation='relu'))\n",
    "    model.add(layers.BatchNormalization())\n",
    "    model.add(layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    for _ in range( {{choice([0, 1, 2, 3])}} ):\n",
    "        model.add(Conv2D(filters=( {{choice([4, 8, 12])}} ), \n",
    "                     kernel_size= (ks2_first, ks2_second), \n",
    "                         padding='same',\n",
    "                     kernel_initializer='TruncatedNormal',\n",
    "                         activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "        model.add(layers.MaxPooling2D((2, 2))) \n",
    "    \n",
    "    model.add(layers.Flatten())\n",
    "    \n",
    "    for _ in range( {{choice([0, 1, 2, 3, 4, 8, 16, 32])}} ):\n",
    "        model.add(Dropout( {{uniform(0, 1)}} ))\n",
    "        model.add(Dense( {{choice([4, 8, 16, 32, 64, 128, 256, 512])}} , \n",
    "                        kernel_initializer='TruncatedNormal',\n",
    "                       activation='relu'))\n",
    "        model.add(layers.BatchNormalization())\n",
    "       \n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer={{choice([\n",
    "                      optimizers.RMSprop(lr=1e-4),\n",
    "                      'Adam',\n",
    "                      'adadelta',\n",
    "                      'adagrad',\n",
    "                      'nadam'\n",
    "                  ])}},\n",
    "                  metrics=['acc'])\n",
    "    \n",
    "    early_stopping_monitor = EarlyStopping(patience=100)\n",
    "    \n",
    "#     schedule = SGDRScheduler(min_lr= {{uniform(1e-8, 1e-5)}} ,\n",
    "#                                      max_lr= {{uniform(1e-3, 1e-1)}} ,\n",
    "#                                      steps_per_epoch=np.ceil(epoch_size/bs),\n",
    "#                                      lr_decay=0.9,\n",
    "#                                      cycle_length=5, # 5\n",
    "#                                      mult_factor=1.5)\n",
    "    checkpoint = ModelCheckpoint(\n",
    "        'model_with_hyperas.h5', \n",
    "        monitor='val_acc', \n",
    "        verbose=0, \n",
    "        save_best_only=True, \n",
    "        save_weights_only=False,\n",
    "        mode='auto'\n",
    "    )\n",
    "\n",
    "    STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
    "    STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
    "\n",
    "    result = model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=STEP_SIZE_TRAIN, \n",
    "        epochs=1,\n",
    "        validation_data=valid_generator,\n",
    "        validation_steps=STEP_SIZE_VALID,\n",
    "        callbacks=[early_stopping_monitor, checkpoint],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    val_acc = np.amin(result.history['val_acc'])\n",
    "    print('Best validation accuracy of epoch:', val_acc)\n",
    "    \n",
    "    return {'loss': -val_acc, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> Imports:\n",
      "#coding=utf-8\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from scipy import ndimage\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import models\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import layers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D, Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import optimizers\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras_preprocessing.image import ImageDataGenerator\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import keras\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras import backend as K\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.keras.applications import DenseNet121\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications.xception import Xception\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications import VGG16\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.applications import NASNetLarge, InceptionV3\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.utils.vis_utils import plot_model\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.models import Sequential\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.recurrent import LSTM\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers import Dense, Conv1D, MaxPool2D, Flatten, Dropout, CuDNNLSTM, CuDNNGRU, Conv2D, MaxPooling2D\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.callbacks import EarlyStopping, TensorBoard, ModelCheckpoint\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.optimizers import Adam, SGD, Nadam\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.layers.normalization import BatchNormalization\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from time import time\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.client import device_lib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.metrics import classification_report, confusion_matrix\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from keras.preprocessing import image\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os, shutil\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from IPython.display import display\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import os\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import cv2\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import math\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import json\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import numpy as np\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import pandas as pd\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas import optim\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperas.distributions import choice, uniform\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from hyperopt import Trials, STATUS_OK, tpe\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from tensorflow.python.client import device_lib\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import itertools\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.image as mpimg\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    from sklearn.model_selection import train_test_split\n",
      "except:\n",
      "    pass\n",
      "\n",
      "try:\n",
      "    import matplotlib.pyplot as plt\n",
      "except:\n",
      "    pass\n",
      "\n",
      ">>> Hyperas search space:\n",
      "\n",
      "def get_space():\n",
      "    return {\n",
      "        'ks1_first': hp.choice('ks1_first', [2, 3, 4, 5, 8, 10]),\n",
      "        'ks1_first_1': hp.choice('ks1_first_1', [2, 3, 4, 5, 8, 10]),\n",
      "        'ks1_first_2': hp.choice('ks1_first_2', [2, 3, 4, 5, 8, 10]),\n",
      "        'ks1_first_3': hp.choice('ks1_first_3', [2, 3, 4, 5, 8, 10]),\n",
      "        'ks1_first_4': hp.choice('ks1_first_4', [1, 2, 3, 4, 5, 8, 12]),\n",
      "        'range': hp.choice('range', [0, 1, 2, 3]),\n",
      "        'range_1': hp.choice('range_1', [4, 8, 12]),\n",
      "        'range_2': hp.choice('range_2', [0, 1, 2, 3, 4, 8, 16, 32]),\n",
      "        'Dropout': hp.uniform('Dropout', 0, 1),\n",
      "        'Dense': hp.choice('Dense', [4, 8, 16, 32, 64, 128, 256, 512]),\n",
      "        'optimizer': hp.choice('optimizer', [\n",
      "                      optimizers.RMSprop(lr=1e-4),\n",
      "                      'Adam',\n",
      "                      'adadelta',\n",
      "                      'adagrad',\n",
      "                      'nadam'\n",
      "                  ]),\n",
      "        'min_lr': hp.uniform('min_lr', 1e-8, 1e-5),\n",
      "        'max_lr': hp.uniform('max_lr', 1e-3, 1e-1),\n",
      "    }\n",
      "\n",
      ">>> Data\n",
      "   1: \n",
      "   2: test_df = pd.read_csv('../pet_data/test/test.csv')\n",
      "   3: train_df = pd.read_csv('../pet_data/train.csv')\n",
      "   4: train_folder = '../pet_data/train_images/'\n",
      "   5: test_folder = '../pet_data/test_images/'\n",
      "   6: name_target_dict = train_df.set_index('PetID')['AdoptionSpeed'].to_dict()\n",
      "   7: name_target_dict_type = train_df.set_index('PetID')['Type'].to_dict()\n",
      "   8: train_image_names = os.listdir('../pet_data/train_images')\n",
      "   9: test_image_names = os.listdir('../pet_data/test_images')\n",
      "  10: generator_dict = {'filename': [], 'class': [], 'type': []}\n",
      "  11: \n",
      "  12: for name in train_image_names:\n",
      "  13:     short_name = name.split('-')[0]\n",
      "  14:     label = name_target_dict[short_name]\n",
      "  15:     t = name_target_dict_type[short_name]\n",
      "  16: \n",
      "  17:     generator_dict['filename'].append(name)\n",
      "  18:     generator_dict['class'].append(label)\n",
      "  19:     generator_dict['type'].append(str(t))\n",
      "  20:     \n",
      "  21: generator_df = pd.DataFrame(generator_dict)\n",
      "  22: generator_df.head(8)\n",
      "  23: \n",
      "  24: random_state = 42\n",
      "  25: train, test = train_test_split(generator_df, test_size=0.2, random_state=random_state)\n",
      "  26: \n",
      "  27: train_datagen=ImageDataGenerator(\n",
      "  28:     featurewise_center=False,  # set input mean to 0 over the dataset\n",
      "  29:     samplewise_center=False,  # set each sample mean to 0\n",
      "  30:     featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
      "  31:     samplewise_std_normalization=False,  # divide each input by its std\n",
      "  32:     zca_whitening=False,  # apply ZCA whitening\n",
      "  33:     zca_epsilon=1e-06,  # epsilon for ZCA whitening\n",
      "  34:     rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
      "  35:     width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
      "  36:     height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
      "  37:     shear_range=0.1,  # set range for random shear\n",
      "  38:     zoom_range=0.1,  # set range for random zoom\n",
      "  39:     channel_shift_range=0.,  # set range for random channel shifts\n",
      "  40:     # set mode for filling points outside the input boundaries\n",
      "  41:     fill_mode='nearest',\n",
      "  42:     cval=0.,  # value used for fill_mode = \"constant\"\n",
      "  43:     horizontal_flip=True,  # randomly flip images\n",
      "  44:     vertical_flip=False,  # randomly flip images\n",
      "  45:     # set rescaling factor (applied before any other transformation)\n",
      "  46:     rescale=1/255.,\n",
      "  47:     # set function that will be applied on each input\n",
      "  48:     preprocessing_function=None,\n",
      "  49:     # fraction of images reserved for validation (strictly between 0 and 1)\n",
      "  50:     validation_split=0.1\n",
      "  51: )\n",
      "  52: \n",
      "  53: train_generator = train_datagen.flow_from_dataframe(\n",
      "  54:     train, \n",
      "  55:     '../pet_data/train_images/', \n",
      "  56:     x_col='filename',\n",
      "  57:     y_col='type', \n",
      "  58:     has_ext=True,  # If image extension is given in x_col\n",
      "  59:     target_size=(150, 150), \n",
      "  60:     color_mode='rgb',\n",
      "  61:     class_mode='binary', \n",
      "  62:     batch_size=16, \n",
      "  63:     shuffle=True, \n",
      "  64:     seed=2018,\n",
      "  65:     subset='training'\n",
      "  66: )\n",
      "  67: valid_generator = train_datagen.flow_from_dataframe(\n",
      "  68:     train, \n",
      "  69:     '../pet_data/train_images/', \n",
      "  70:     x_col='filename',\n",
      "  71:     y_col='type', \n",
      "  72:     has_ext=True,  # If image extension is given in x_col\n",
      "  73:     target_size=(150, 150), \n",
      "  74:     color_mode='rgb',\n",
      "  75:     class_mode='binary', \n",
      "  76:     batch_size=16, \n",
      "  77:     shuffle=True, \n",
      "  78:     seed=2018,\n",
      "  79:     subset='validation'\n",
      "  80: )\n",
      "  81: \n",
      "  82: \n",
      "  83: \n",
      "  84: \n",
      ">>> Resulting replaced keras model:\n",
      "\n",
      "   1: def keras_fmin_fnct(space):\n",
      "   2: \n",
      "   3:     model = models.Sequential()\n",
      "   4:     \n",
      "   5:     ks1_first = space['ks1_first']\n",
      "   6:     ks1_second = space['ks1_first_1']\n",
      "   7:     \n",
      "   8:     ks2_first = space['ks1_first_2']\n",
      "   9:     ks2_second = space['ks1_first_3']\n",
      "  10:     \n",
      "  11:     input_shape=(150, 150, 3)\n",
      "  12:     \n",
      "  13:     model.add(Conv2D(filters=( space['ks1_first_4'] ), \n",
      "  14:                      kernel_size=(ks1_first, ks1_second),\n",
      "  15:                      input_shape=input_shape, \n",
      "  16:                      padding='same',\n",
      "  17:                      kernel_initializer='TruncatedNormal',\n",
      "  18:                      activation='relu'))\n",
      "  19:     model.add(layers.BatchNormalization())\n",
      "  20:     model.add(layers.MaxPooling2D((2, 2)))\n",
      "  21:     \n",
      "  22:     for _ in range( space['range'] ):\n",
      "  23:         model.add(Conv2D(filters=( space['range_1'] ), \n",
      "  24:                      kernel_size= (ks2_first, ks2_second), \n",
      "  25:                          padding='same',\n",
      "  26:                      kernel_initializer='TruncatedNormal',\n",
      "  27:                          activation='relu'))\n",
      "  28:         model.add(layers.BatchNormalization())\n",
      "  29:         model.add(layers.MaxPooling2D((2, 2))) \n",
      "  30:     \n",
      "  31:     model.add(layers.Flatten())\n",
      "  32:     \n",
      "  33:     for _ in range( space['range_2'] ):\n",
      "  34:         model.add(Dropout( space['Dropout'] ))\n",
      "  35:         model.add(Dense( space['Dense'] , \n",
      "  36:                         kernel_initializer='TruncatedNormal',\n",
      "  37:                        activation='relu'))\n",
      "  38:         model.add(layers.BatchNormalization())\n",
      "  39:        \n",
      "  40:     model.add(layers.Dense(1, activation='sigmoid'))\n",
      "  41:     model.compile(loss='binary_crossentropy',\n",
      "  42:                   optimizer=space['optimizer'],\n",
      "  43:                   metrics=['acc'])\n",
      "  44:     \n",
      "  45:     early_stopping_monitor = EarlyStopping(patience=100)\n",
      "  46:     \n",
      "  47: #     schedule = SGDRScheduler(min_lr= space['min_lr'] ,\n",
      "  48: #                                      max_lr= space['max_lr'] ,\n",
      "  49: #                                      steps_per_epoch=np.ceil(epoch_size/bs),\n",
      "  50: #                                      lr_decay=0.9,\n",
      "  51: #                                      cycle_length=5, # 5\n",
      "  52: #                                      mult_factor=1.5)\n",
      "  53:     checkpoint = ModelCheckpoint(\n",
      "  54:         'model_with_hyperas.h5', \n",
      "  55:         monitor='val_acc', \n",
      "  56:         verbose=0, \n",
      "  57:         save_best_only=True, \n",
      "  58:         save_weights_only=False,\n",
      "  59:         mode='auto'\n",
      "  60:     )\n",
      "  61: \n",
      "  62:     STEP_SIZE_TRAIN = train_generator.n//train_generator.batch_size\n",
      "  63:     STEP_SIZE_VALID = valid_generator.n//valid_generator.batch_size\n",
      "  64: \n",
      "  65:     result = model.fit_generator(\n",
      "  66:         train_generator,\n",
      "  67:         steps_per_epoch=STEP_SIZE_TRAIN, \n",
      "  68:         epochs=1,\n",
      "  69:         validation_data=valid_generator,\n",
      "  70:         validation_steps=STEP_SIZE_VALID,\n",
      "  71:         callbacks=[early_stopping_monitor, checkpoint],\n",
      "  72:         verbose=0\n",
      "  73:     )\n",
      "  74:     \n",
      "  75:     val_acc = np.amin(result.history['val_acc'])\n",
      "  76:     print('Best validation accuracy of epoch:', val_acc)\n",
      "  77:     \n",
      "  78:     return {'loss': -val_acc, 'status': STATUS_OK, 'model': model}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  79: \n",
      "Found 7438 images belonging to 2 classes.\n",
      "Found 826 images belonging to 2 classes.\n",
      "WARNING:tensorflow:From C:\\Users\\darre\\Anaconda3\\envs\\project_new\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "  0%|          | 0/5 [00:00<?, ?it/s, best loss: ?]WARNING:tensorflow:From C:\\Users\\darre\\Anaconda3\\envs\\project_new\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\Users\\darre\\Anaconda3\\envs\\project_new\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Best validation accuracy of epoch:                 \n",
      "0.5281862745098039                                 \n",
      "Best validation accuracy of epoch:                                            \n",
      "0.491358024764944                                                             \n",
      "Best validation accuracy of epoch:                                            \n",
      "0.528395061801981                                                             \n",
      "Best validation accuracy of epoch:                                            \n",
      "0.5271604941215045                                                           \n",
      "Best validation accuracy of epoch:                                           \n",
      "0.5111111109639391                                                           \n",
      "100%|██████████| 5/5 [10:11<00:00, 124.16s/it, best loss: -0.528395061801981]\n",
      "Found 7438 images belonging to 2 classes.\n",
      "Found 826 images belonging to 2 classes.\n",
      "Evalutation of best performing model:\n",
      "Crossvalidation of best performing model:\n",
      "Best performing model chosen hyper-parameters:\n",
      "{'Dense': 32, 'Dropout': 0.9970627896910466, 'ks1_first': 5, 'ks1_first_1': 3, 'ks1_first_2': 8, 'ks1_first_3': 10, 'ks1_first_4': 1, 'max_lr': 0.040104369039604224, 'min_lr': 9.758426998273488e-06, 'optimizer': 'nadam', 'range': 2, 'range_1': 12, 'range_2': 8}\n"
     ]
    }
   ],
   "source": [
    "best_run, best_model = optim.minimize(\n",
    "    model=model, \n",
    "    data=data,\n",
    "    algo=tpe.suggest,\n",
    "    max_evals=5,\n",
    "    eval_space=True,\n",
    "    trials=Trials(),\n",
    "    notebook_name='Train cat dog classifier hyperparam tuning with augmentation'\n",
    ")\n",
    "    \n",
    "X_train, y_train, X_test, y_test = data()\n",
    "print(\"Evalutation of best performing model:\")\n",
    "# print(best_model.evaluate(valid))\n",
    "\n",
    "print(\"Crossvalidation of best performing model:\")\n",
    "#print(best_model.evaluate(X_test, y_test))\n",
    "\n",
    "print(\"Best performing model chosen hyper-parameters:\")\n",
    "print(best_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
